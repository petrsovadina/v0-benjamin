{
  "file_path": "backend/pipeline/run_pipeline.py",
  "main_branch_history": [],
  "task_views": {
    "001-complete-guidelines-rag-pdf-import": {
      "task_id": "001-complete-guidelines-rag-pdf-import",
      "branch_point": {
        "commit_hash": "d092d181e511c0ef7a36dcfd2dfe3f083cbae73e",
        "content": "",
        "timestamp": "2025-12-25T01:35:52.079046"
      },
      "worktree_state": {
        "content": "import argparse\nimport asyncio\nimport logging\nfrom pathlib import Path\n\nfrom backend.data_processing.config.settings import settings\n\n# Imports from new structure\nfrom backend.data_processing.parsers.sukl_dlp_parser import SuklDlpParser\nfrom backend.data_processing.loaders.drug_loader import DrugLoader\n\nfrom backend.data_processing.parsers.sukl_pricing_parser import SuklPricingParser\nfrom backend.data_processing.loaders.pricing_loader import PricingLoader\n\nfrom backend.data_processing.parsers.spc_pil_parser import SpcPilParser\nfrom backend.data_processing.loaders.document_loader import DocumentLoader\nfrom backend.data_processing.embeddings.embedding_generator import EmbeddingGenerator\nfrom backend.data_processing.downloaders.sukl_downloader import SuklDownloader\n\n# Setup logging\nlogging.basicConfig(level=logging.INFO)\nlogger = logging.getLogger(__name__)\n\nasync def main():\n    parser = argparse.ArgumentParser(description=\"run_pipeline\")\n    parser.add_argument(\"--download\", action=\"store_true\", help=\"Download raw CSVs\")\n    parser.add_argument(\"--drugs\", action=\"store_true\", help=\"Run Drug pipeline (DLP)\")\n    parser.add_argument(\"--pricing\", action=\"store_true\", help=\"Run Pricing pipeline\")\n    parser.add_argument(\"--embeddings\", action=\"store_true\", help=\"Process embeddings (separate mode, not yet implemented)\")\n    parser.add_argument(\"--with-embeddings\", action=\"store_true\", help=\"Enable vector embedding generation during drug processing\")\n    parser.add_argument(\"--documents\", action=\"store_true\", help=\"Generate SPC/PIL metadata\")\n    parser.add_argument(\"--guidelines\", action=\"store_true\", help=\"Run Guidelines Ingestion (PDFs)\")\n    parser.add_argument(\"--substances\", action=\"store_true\", help=\"Run Active Substances pipeline\")\n    parser.add_argument(\"--limit\", type=int, default=None, help=\"Limit items processed\")\n    parser.add_argument(\"--all\", action=\"store_true\", help=\"Run full pipeline\")\n    \n    args = parser.parse_args()\n\n    # Paths (using settings)\n    raw_data_path = settings.RAW_DATA_DIR\n    dlp_path = raw_data_path / \"dlp_leciva.csv\"\n    pricing_path = raw_data_path / \"scau_leciva.csv\"\n    substances_path = raw_data_path / \"nkod_dlp_lecivelatky.csv\"\n\n    # Default to all if no specific flag\n    if args.all or (not args.download and not args.drugs and not args.pricing and not args.documents and not args.guidelines and not args.substances):\n        args.download = True\n        args.drugs = True\n        args.pricing = True\n        args.documents = True\n        args.guidelines = True\n        args.substances = True\n\n    if args.download:\n        logger.info(\"--- Downloading Data ---\")\n        downloader = SuklDownloader()\n        await downloader.download_all()\n\n        # 2b. Download Documents Mapping & Auxiliary Files\n        await downloader.download_with_retry(\"dlp_monthly\", \"dlp_nazvydokumentu.csv\", file_pattern=\"nazvydokumentu\")\n        await downloader.download_with_retry(\"dlp_monthly\", \"dlp_atc.csv\", file_pattern=\"dlp_atc.csv\")\n        await downloader.download_with_retry(\"dlp_monthly\", \"dlp_latky.csv\", file_pattern=\"dlp_latky.csv\")\n\n    if args.substances:\n         logger.info(\"--- Running Active Substances Pipeline ---\")\n         if not substances_path.exists():\n             logger.warning(f\"Active substances file {substances_path} not found. Skipping.\")\n         else:\n             from backend.data_processing.parsers.active_substance_parser import ActiveSubstanceParser\n             from backend.data_processing.loaders.active_substance_loader import ActiveSubstanceLoader\n             \n             parser = ActiveSubstanceParser(str(substances_path))\n             items = parser.parse(limit=args.limit)\n             loader = ActiveSubstanceLoader()\n             loader.load_substances(items)\n\n    if args.drugs:\n        logger.info(\"--- Running Drug Pipeline ---\")\n        # Initialize EmbeddingGenerator for search text creation (and embeddings if key present)\n        from backend.data_processing.generators.embedding_generator import EmbeddingGenerator\n        emb_gen = EmbeddingGenerator()\n        should_embed = args.with_embeddings and settings.OPENAI_API_KEY\n\n        if args.with_embeddings and not settings.OPENAI_API_KEY:\n            logger.warning(\"OPENAI_API_KEY not set. Skipping embedding generation despite --with-embeddings flag.\")\n        \n        from backend.data_processing.parsers.auxiliary_parser import AuxiliaryParser\n        \n        # 1. Load Auxiliary Maps (ATC, Active Substances)\n        atc_path = raw_data_path / \"dlp_atc.csv\"\n        dlp_latky_path = raw_data_path / \"dlp_latky.csv\"\n\n        aux_parser = AuxiliaryParser()\n        atc_map = {}\n        if atc_path.exists():\n            logger.info(f\"Loading ATC map from {atc_path}\")\n            atc_map = aux_parser.parse_atc(str(atc_path))\n        else:\n            logger.warning(f\"ATC file {atc_path} not found. ATC names will not be mapped.\")\n\n        active_substance_map = {}\n        if dlp_latky_path.exists():\n            logger.info(f\"Loading Active Substances map from {dlp_latky_path}\")\n            active_substance_map = aux_parser.parse_substances(str(dlp_latky_path))\n        else:\n            logger.warning(f\"DLP Latky file {dlp_latky_path} not found. Active substances will not be mapped.\")\n\n        drug_loader = DrugLoader()\n        \n        # Process Monthly DLP\n        if dlp_path.exists():\n            logger.info(f\"Processing Monthly DLP: {dlp_path}\")\n            parser = SuklDlpParser(str(dlp_path))\n            items = parser.parse(limit=args.limit)\n            \n            # Enrich and load in batches\n            batch_size = 100\n            for i in range(0, len(items), batch_size):\n                batch = items[i:i + batch_size]\n                \n                # Enrich with ATC names and active substances\n                for item in batch:\n                    item['atc_name'] = atc_map.get(item.get('atc_code'), \"\")\n                    # active_substances from map? or raw? current parser gets generic list?\n                    # Auxiliary map returns list of dicts. We need mapping by CODE? \n                    # Sukl parser has 'sukl_code'. Does latky have sukl_code?\n                    # dlp_latky has KOD_LATKY. Mapping KOD_SUKL -> KOD_LATKY is in dlp_slozeni.csv?\n                    # The user guide said \"Auxiliary Data: Ingest dlp_atc.csv, dlp_latky.csv\".\n                    # Direct mapping might be tricky without dlp_slozeni.\n                    # For now I will proceed with just ATC mapping and generating search text from existing item fields.\n                    \n                    item['search_text'] = emb_gen.create_search_text(item, item['atc_name'])\n\n                # Generate embeddings for the batch if enabled\n                if should_embed:\n                    texts_to_embed = [item['search_text'] for item in batch if item.get('search_text')]\n                    if texts_to_embed:\n                        try:\n                            embeddings = emb_gen.generate_embeddings(texts_to_embed)\n                            for j, item in enumerate(batch):\n                                if item.get('search_text'):\n                                    item['embedding'] = embeddings[j]\n                        except Exception as e:\n                            logger.error(f\"Embedding failed for batch {i}: {e}\")\n                    else:\n                        logger.warning(f\"No search text to embed for batch starting at index {i}.\")\n\n                # Load the batch\n                drug_loader.load_drugs(batch)\n                logger.info(f\"Processed and loaded {min(batch_size, len(items) - i)} drugs (Monthly DLP).\")\n        else:\n             logger.warning(f\"Monthly DLP file not found at {dlp_path}\")\n\n        # Process Current DLP (eRecept) - Upsert to get latest state\n        erecept_path = raw_data_path / \"dlp_erecept.csv\"\n        if erecept_path.exists():\n            logger.info(f\"Processing Current DLP (eRecept): {erecept_path}\")\n            parser = SuklDlpParser(str(erecept_path))\n            items = parser.parse(limit=args.limit)\n\n            # Enrich and load in batches\n            batch_size = 100\n            for i in range(0, len(items), batch_size):\n                batch = items[i:i + batch_size]\n                \n                # Enrich with ATC names and active substances\n                for item in batch:\n                    item['atc_name'] = atc_map.get(item.get('atc_code'), \"\")\n                    # Same logic for active substances - simplified for now\n                    \n                    item['search_text'] = emb_gen.create_search_text(item, item['atc_name'])\n\n                # Generate embeddings for the batch if enabled\n                if should_embed:\n                    texts_to_embed = [item['search_text'] for item in batch if item.get('search_text')]\n                    if texts_to_embed:\n                        try:\n                            embeddings = emb_gen.generate_embeddings(texts_to_embed)\n                            for j, item in enumerate(batch):\n                                if item.get('search_text'):\n                                    item['embedding'] = embeddings[j]\n                        except Exception as e:\n                            logger.error(f\"Embedding failed for eRecept batch {i}: {e}\")\n                    else:\n                        logger.warning(f\"No search text to embed for eRecept batch starting at index {i}.\")\n\n                # Load the batch\n                drug_loader.load_drugs(batch)\n                logger.info(f\"Processed and loaded {min(batch_size, len(items) - i)} drugs (eRecept DLP).\")\n        else:\n             logger.info(f\"Current DLP file not found at {erecept_path}\")\n\n    if args.pricing:\n        logger.info(\"--- Running Pricing Pipeline ---\")\n        \n        # 1. Current Pricing (Update Drugs table)\n        if pricing_path.exists():\n            logger.info(f\"Processing Current Pricing: {pricing_path}\")\n            parser = SuklPricingParser()\n            items = parser.parse_pricing(pricing_path)\n            if args.limit:\n                items = items[:args.limit]\n            loader = PricingLoader()\n            loader.load_pricing(items)\n        else:\n            logger.warning(f\"Pricing file {pricing_path} not found. Skipping.\")\n\n        # 2. Historical Pricing (Archives)\n        # Assuming we want to process archives if they exist, or via flag?\n        # Let's stick to explicit flag --archives or just checking file existence if user downloaded them\n        # Implementation: Check for 2024, 2023 zips\n        \n        archives = [\n            raw_data_path / \"lek13_2024.zip\",\n            raw_data_path / \"lek13_2023.zip\",\n            raw_data_path / \"lek13_2025_10.csv\"\n        ]\n        \n        # Only process if present\n        found_archives = [p for p in archives if p.exists()]\n        \n        if found_archives:\n            logger.info(f\"Found {len(found_archives)} archives/historical files. Processing...\")\n            from backend.data_processing.loaders.price_history_loader import PriceHistoryLoader\n            \n            history_loader = PriceHistoryLoader()\n            parser = SuklPricingParser()\n            \n            for arch_path in found_archives:\n                logger.info(f\"Parsing archive: {arch_path}\")\n                history_items = parser.parse_pricing(arch_path)\n                \n                if args.limit:\n                    history_items = history_items[:args.limit]\n                \n                history_loader.load_price_history(history_items)\n        else:\n            logger.info(\"No pricing archives found. Use --download to fetch them if needed.\")\n\n    if args.documents:\n        logger.info(\"--- Running Documents Mapping Pipeline ---\")\n        \n        # 1. Parse Mapping\n        mapping_file = raw_data_path / \"dlp_nazvydokumentu.csv\"\n        if mapping_file.exists():\n            from backend.data_processing.parsers.sukl_document_parser import SuklDocumentParser\n            from backend.data_processing.loaders.document_mapping_loader import DocumentMappingLoader\n            \n            parser = SuklDocumentParser()\n            items = parser.parse_mapping(mapping_file)\n            \n            if args.limit:\n                items = items[:args.limit]\n                \n            loader = DocumentMappingLoader()\n            loader.load_mapping(items)\n        else:\n            logger.warning(f\"Document mapping file {mapping_file} not found. Run --download.\")\n\n    if args.guidelines:\n        logger.info(\"--- Running Guidelines Pipeline ---\")\n        from backend.data_processing.loaders.guidelines_loader import GuidelinesLoader\n        loader = GuidelinesLoader()\n        await loader.ingest_pdfs()\n\nif __name__ == \"__main__\":\n    asyncio.run(main())\n",
        "last_modified": "2025-12-25T21:22:15.353776"
      },
      "task_intent": {
        "title": "001-complete-guidelines-rag-pdf-import",
        "description": "",
        "from_plan": false
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2025-12-25T01:36:13.951192",
  "last_updated": "2025-12-25T01:36:13.963944"
}