{
  "file_path": "backend/data_processing/loaders/guidelines_loader.py",
  "main_branch_history": [],
  "task_views": {
    "001-complete-guidelines-rag-pdf-import": {
      "task_id": "001-complete-guidelines-rag-pdf-import",
      "branch_point": {
        "commit_hash": "d092d181e511c0ef7a36dcfd2dfe3f083cbae73e",
        "content": "",
        "timestamp": "2025-12-25T01:35:52.079046"
      },
      "worktree_state": {
        "content": "import os\nimport glob\nimport time\nfrom typing import List, Optional\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\nfrom langchain_openai import OpenAIEmbeddings\nfrom supabase import Client\n\nfrom backend.app.core.config import settings\nfrom backend.app.core.database import get_supabase_client\nfrom backend.services.logger import get_logger\n\nlogger = get_logger(__name__)\n\n# Retry configuration for embedding generation\nEMBEDDING_MAX_RETRIES = 3\nEMBEDDING_BASE_DELAY = 1.0  # seconds\nEMBEDDING_MAX_DELAY = 10.0  # seconds\n\nclass GuidelinesLoader:\n    \"\"\"\n    Handles loading, chunking, and embedding of Guideline PDFs.\n    \"\"\"\n    \n    def __init__(self, pdf_dir: str = \"backend/data/guidelines_pdfs\"):\n        self.pdf_dir = pdf_dir\n        self.supabase: Client = get_supabase_client()\n        self.embeddings = OpenAIEmbeddings(\n            model=\"text-embedding-3-small\", \n            api_key=settings.OPENAI_API_KEY\n        )\n        self.text_splitter = RecursiveCharacterTextSplitter(\n            chunk_size=1000,\n            chunk_overlap=200,\n            separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n        )\n\n    def _embed_with_retry(self, texts: List[str], batch_index: int, filename: str) -> List[List[float]]:\n        \"\"\"\n        Generate embeddings with exponential backoff retry logic.\n\n        Args:\n            texts: List of text chunks to embed\n            batch_index: Current batch index for logging\n            filename: Source filename for logging context\n\n        Returns:\n            List of embedding vectors\n\n        Raises:\n            Exception: If all retries are exhausted\n        \"\"\"\n        last_exception = None\n\n        for attempt in range(1, EMBEDDING_MAX_RETRIES + 1):\n            try:\n                vectors = self.embeddings.embed_documents(texts)\n                if attempt > 1:\n                    logger.info(\n                        \"Embedding generation succeeded after retry\",\n                        filename=filename,\n                        step=\"embed_with_retry\",\n                        batch_index=batch_index,\n                        attempt=attempt\n                    )\n                return vectors\n            except Exception as e:\n                last_exception = e\n\n                if attempt < EMBEDDING_MAX_RETRIES:\n                    # Calculate delay with exponential backoff\n                    delay = min(EMBEDDING_BASE_DELAY * (2 ** (attempt - 1)), EMBEDDING_MAX_DELAY)\n\n                    logger.warning(\n                        \"Embedding generation failed, retrying\",\n                        error_message=str(e),\n                        filename=filename,\n                        step=\"embed_with_retry\",\n                        batch_index=batch_index,\n                        texts_count=len(texts),\n                        attempt=attempt,\n                        max_retries=EMBEDDING_MAX_RETRIES,\n                        retry_delay_seconds=delay\n                    )\n\n                    time.sleep(delay)\n                else:\n                    logger.error(\n                        \"Embedding generation failed after max retries\",\n                        error=last_exception,\n                        filename=filename,\n                        step=\"embed_with_retry\",\n                        batch_index=batch_index,\n                        texts_count=len(texts),\n                        max_retries=EMBEDDING_MAX_RETRIES\n                    )\n\n        # Re-raise the last exception if all retries exhausted\n        raise last_exception\n\n    async def ingest_pdfs(self):\n        \"\"\"\n        Scans the PDF directory, processes each file, and upserts chunks to Supabase.\n        \"\"\"\n        pdf_files = glob.glob(os.path.join(self.pdf_dir, \"*.pdf\"))\n        \n        if not pdf_files:\n            logger.warning(\n                \"No PDF files found in directory\",\n                pdf_dir=self.pdf_dir,\n                step=\"ingest_pdfs\"\n            )\n            return\n\n        logger.info(\n            \"Found PDF files to process\",\n            pdf_count=len(pdf_files),\n            pdf_dir=self.pdf_dir,\n            step=\"ingest_pdfs\"\n        )\n\n        total_chunks = 0\n        \n        for file_path in pdf_files:\n            filename = os.path.basename(file_path)\n            logger.info(\n                \"Processing PDF file\",\n                filename=filename,\n                file_path=file_path,\n                step=\"ingest_pdfs\"\n            )\n            \n            try:\n                # 1. Load PDF\n                loader = PyPDFLoader(file_path)\n                docs = loader.load()\n                \n                # 2. Split into chunks\n                chunks = self.text_splitter.split_documents(docs)\n                logger.info(\n                    \"Split PDF into chunks\",\n                    filename=filename,\n                    chunk_count=len(chunks),\n                    step=\"split_documents\"\n                )\n                \n                # 3. Generate Embeddings & Prepare for DB\n                records = []\n                # We process in batches to avoid hitting API limits or huge payloads\n                batch_size = 50\n                \n                for i in range(0, len(chunks), batch_size):\n                    batch = chunks[i:i+batch_size]\n                    batch_texts = [c.page_content for c in batch]\n                    batch_index = i // batch_size\n\n                    # Generate embeddings with retry logic\n                    vectors = self._embed_with_retry(batch_texts, batch_index, filename)\n\n                    for j, chunk in enumerate(batch):\n                        records.append({\n                            # Required fields by 008_guidelines.sql\n                            \"title\": filename,\n                            \"organization\": \"Unknown\",\n                            \"publication_year\": \"2024\",  # TEXT type in schema\n                            \"is_czech\": True,\n                            # Chunk content for RAG retrieval\n                            \"content\": chunk.page_content,\n                            # Metadata JSONB for source attribution and citations\n                            \"metadata\": {\n                                \"source\": filename,\n                                \"page\": chunk.metadata.get(\"page\", 0),\n                                **chunk.metadata\n                            },\n                            \"embedding\": vectors[j]\n                        })\n                \n                # 4. Upsert to Supabase\n                # Note: 'guidelines' table must exist with (id, content, metadata, embedding)\n                # We let Supabase generate IDs (if table is set up that way) or we ignore conflicts?\n                # Usually standard RAG doesn't have a unique key other than ID. \n                # We will just insert them. To avoid duplicates on re-run, we might want to delete old ones for this file first.\n                \n                # Delete existing chunks for this file to ensure idempotency\n                self.supabase.table(\"guidelines\").delete().filter(\"metadata->>source\", \"eq\", filename).execute()\n                \n                # Insert new ones in batches\n                for i in range(0, len(records), batch_size):\n                    self.supabase.table(\"guidelines\").insert(records[i:i+batch_size]).execute()\n                \n                logger.info(\n                    \"Successfully stored chunks in database\",\n                    filename=filename,\n                    chunks_stored=len(records),\n                    step=\"store_to_database\"\n                )\n                total_chunks += len(records)\n                \n            except Exception as e:\n                logger.error(\n                    \"Failed to process PDF file\",\n                    error=e,\n                    filename=filename,\n                    step=\"ingest_pdfs\",\n                    file_path=file_path\n                )\n                \n        logger.info(\n            \"Ingestion complete\",\n            total_chunks_stored=total_chunks,\n            pdf_count=len(pdf_files),\n            step=\"ingest_pdfs\"\n        )\n",
        "last_modified": "2025-12-25T21:22:14.907162"
      },
      "task_intent": {
        "title": "001-complete-guidelines-rag-pdf-import",
        "description": "",
        "from_plan": false
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2025-12-25T01:36:13.266295",
  "last_updated": "2025-12-25T01:36:13.280778"
}