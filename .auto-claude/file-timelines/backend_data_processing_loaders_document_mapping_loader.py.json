{
  "file_path": "backend/data_processing/loaders/document_mapping_loader.py",
  "main_branch_history": [],
  "task_views": {
    "001-complete-guidelines-rag-pdf-import": {
      "task_id": "001-complete-guidelines-rag-pdf-import",
      "branch_point": {
        "commit_hash": "d092d181e511c0ef7a36dcfd2dfe3f083cbae73e",
        "content": "",
        "timestamp": "2025-12-25T01:35:52.079046"
      },
      "worktree_state": {
        "content": "from typing import List, Dict, Any\nimport logging\nfrom backend.data_processing.utils.supabase_client import SupabaseSingleton\n\nlogger = logging.getLogger(__name__)\n\nclass DocumentMappingLoader:\n    \"\"\"\n    Updates 'drugs' table with spc_file and pil_file names.\n    \"\"\"\n    def __init__(self):\n        self.supabase = SupabaseSingleton.get_client()\n\n    def load_mapping(self, mapping_items: List[Dict[str, Any]], batch_size: int = 100):\n        total = len(mapping_items)\n        logger.info(f\"Starting update of document mapping for {total} items...\")\n        \n        # 1. Fetch existing SUKL codes to avoid inserting ghosts (which fails on name NOT NULL)\n        # 60k is manageable. Use pagination or large CSV export if needed, but simple select works for <100k\n        # 1. Fetch existing SUKL codes and names to avoid inserting ghosts and satisfy NOT NULL name\n        try:\n            logger.info(\"Fetching existing SUKL codes and names from DB...\")\n            existing_drugs = {} # sukl_code -> name\n            count = 0 \n            page_size = 1000\n            \n            # Simple paging loop\n            while True:\n                res = self.supabase.table(\"drugs\").select(\"sukl_code, name\").range(count, count + page_size - 1).execute()\n                batch_data = res.data\n                if not batch_data:\n                    break\n                for r in batch_data:\n                    if r.get('sukl_code'):\n                        existing_drugs[r['sukl_code']] = r.get('name', 'Nezn\u00e1m\u00fd')\n                \n                count += len(batch_data)\n                if len(batch_data) < page_size:\n                    break\n            \n            logger.info(f\"Loaded {len(existing_drugs)} existing drugs.\")\n        except Exception as e:\n            logger.error(f\"Failed to fetch existing drugs: {e}\")\n            return\n\n        # 2. Filter mapping items and enrich with name\n        valid_items = []\n        for item in mapping_items:\n            code = item[\"sukl_code\"]\n            if code in existing_drugs:\n                # Add name to satisfy constraint\n                item_with_name = item.copy()\n                item_with_name['name'] = existing_drugs[code]\n                valid_items.append(item_with_name)\n\n        logger.info(f\"Filtered {len(valid_items)} valid mapping items (matching existing drugs).\")\n\n        for i in range(0, len(valid_items), batch_size):\n            batch = valid_items[i : i + batch_size]\n            \n            try:\n                data = self.supabase.table(\"drugs\").upsert(\n                    batch, \n                    on_conflict=\"sukl_code\"\n                ).execute()\n                \n                logger.info(f\"Updated document mapping for batch {i//batch_size + 1}/{(len(valid_items)//batch_size) + 1}\")\n            except Exception as e:\n                logger.error(f\"Error updating document mapping batch {i}: {e}\")\n                \n        logger.info(\"Document mapping update complete.\")\n",
        "last_modified": "2025-12-25T21:22:14.904127"
      },
      "task_intent": {
        "title": "001-complete-guidelines-rag-pdf-import",
        "description": "",
        "from_plan": false
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2025-12-25T01:36:13.237638",
  "last_updated": "2025-12-25T01:36:13.250665"
}