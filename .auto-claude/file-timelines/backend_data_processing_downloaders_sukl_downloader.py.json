{
  "file_path": "backend/data_processing/downloaders/sukl_downloader.py",
  "main_branch_history": [],
  "task_views": {
    "001-complete-guidelines-rag-pdf-import": {
      "task_id": "001-complete-guidelines-rag-pdf-import",
      "branch_point": {
        "commit_hash": "d092d181e511c0ef7a36dcfd2dfe3f083cbae73e",
        "content": "",
        "timestamp": "2025-12-25T01:35:52.079046"
      },
      "worktree_state": {
        "content": "import httpx\nimport logging\nimport aiofiles\nfrom pathlib import Path\nfrom backend.data_processing.config.settings import settings\n\nlogger = logging.getLogger(__name__)\n\nclass SuklDownloader:\n    \"\"\"\n    Downloads S\u00daKL datasets (DLP, Pricing) from OpenData.\n    Migrated to backend.data_processing.\n    \"\"\"\n    \n    # URLs verified as of Dec 2025\n    URLS = {\n        \"dlp_monthly\": \"https://opendata.sukl.cz/soubory/SOD20251127/DLP20251127.zip\",\n        \"dlp_current\": \"https://opendata.sukl.cz/soubory/SODERECEPT/DLPAKTUALNI.zip\",\n        \"dlp_interface\": \"https://opendata.sukl.cz/soubory/DLP_datove_rozhrani20250601.csv\",\n        \"substances\": \"https://opendata.sukl.cz/soubory/NKOD/DLP/nkod_dlp_lecivelatky.csv\",\n        \"prices\": \"https://opendata.sukl.cz/soubory/LEK13/LEK13_2025/LEK13_202511v01.csv\",\n        \"lek13_2024\": \"https://opendata.sukl.cz/soubory/LEK13/LEK13_2024/LEK13_2024.zip\",\n        \"lek13_2023\": \"https://opendata.sukl.cz/soubory/LEK13/LEK13_2023/LEK13_2023.zip\",\n        \"lek13_prev_month\": \"https://opendata.sukl.cz/soubory/LEK13/LEK13_2025/LEK13_202510v01.csv\"\n    }\n\n    def __init__(self, download_dir: str = None):\n        if download_dir:\n            self.download_dir = Path(download_dir)\n        else:\n            self.download_dir = settings.RAW_DATA_DIR\n            \n        self.download_dir.mkdir(parents=True, exist_ok=True)\n\n    async def download_all(self):\n        \"\"\"\n        Attempts to download all configured datasets.\n        \"\"\"\n        # 1. Download DLP Monthly (Drugs)\n        await self.download_with_retry(\"dlp_monthly\", \"dlp_leciva.csv\")\n        \n        # 2. Download DLP Current (eRecept)\n        await self.download_with_retry(\"dlp_current\", \"dlp_erecept.csv\")\n\n        # 2b. Download Documents Mapping & Auxiliary Files\n        await self.download_with_retry(\"dlp_monthly\", \"dlp_nazvydokumentu.csv\", file_pattern=\"nazvydokumentu\")\n        await self.download_with_retry(\"dlp_monthly\", \"dlp_atc.csv\", file_pattern=\"dlp_atc.csv\")\n        await self.download_with_retry(\"dlp_monthly\", \"dlp_latky.csv\", file_pattern=\"dlp_latky.csv\")\n\n        # 3. Download Data Interface\n        await self.download_with_retry(\"dlp_interface\", \"dlp_datove_rozhrani.csv\")\n        \n        # 4. Download Active Substances\n        await self.download_with_retry(\"substances\", \"nkod_dlp_lecivelatky.csv\")\n\n        # 5. Download Pricing (Current)\n        await self.download_with_retry(\"prices\", \"scau_leciva.csv\")\n        \n        # 6. Download Pricing Archives (Keep as ZIP)\n        await self.download_with_retry(\"lek13_2024\", \"lek13_2024.zip\", extract=False)\n        await self.download_with_retry(\"lek13_2023\", \"lek13_2023.zip\", extract=False)\n        await self.download_with_retry(\"lek13_prev_month\", \"lek13_2025_10.csv\")\n\n    async def download_with_retry(self, key: str, target_filename: str, extract: bool = True, file_pattern: str = None):\n        url = self.URLS.get(key)\n        if not url:\n            logger.warning(f\"No URL found for {key}\")\n            return\n\n        logger.info(f\"Downloading {key} from {url}...\")\n        try:\n            await self.download_file(url, target_filename, extract=extract, file_pattern=file_pattern)\n        except Exception as e:\n            logger.error(f\"Failed to download {key}: {e}. Ensure URL is correct or file is manually placed in {self.download_dir}\")\n\n    async def download_file(self, url: str, target_filename: str, extract: bool = True, file_pattern: str = None):\n        import zipfile\n        import io\n        \n        async with httpx.AsyncClient(timeout=60.0, follow_redirects=True) as client:\n            resp = await client.get(url)\n            resp.raise_for_status()\n            \n            # Check if it is a zip file and extraction is requested\n            is_zip = url.endswith(\".zip\") or resp.headers.get(\"content-type\") == \"application/zip\"\n            \n            if is_zip and extract:\n                logger.info(f\"Extracting zip from {url}...\")\n                with zipfile.ZipFile(io.BytesIO(resp.content)) as z:\n                    file_list = z.namelist()\n                    logger.info(f\"Files in ZIP: {file_list}\")\n                    \n                    csv_files = [f for f in file_list if f.lower().endswith(\".csv\") or f.lower().endswith(\".txt\")]\n                    if not csv_files:\n                        logger.error(\"No CSV/TXT found in zip\")\n                        return\n                    \n                    source_filename = None\n                    \n                    # 0. Prefer explicit pattern if provided\n                    if file_pattern:\n                        pattern_matches = [f for f in csv_files if file_pattern.lower() in f.lower()]\n                        if pattern_matches:\n                            source_filename = pattern_matches[0]\n                            logger.info(f\"Found file matching pattern '{file_pattern}': {source_filename}\")\n\n                    if not source_filename:\n                        # Heuristic to find the main DLP file\n                        candidates = []\n                        \n                        # 1. Look for 'lecivepripravky' specifically (common in DLP)\n                        priority_files = [f for f in csv_files if \"lecivepripravky\" in f.lower()]\n                        \n                        if priority_files:\n                            source_filename = priority_files[0]\n                        else:\n                            # 2. Filtering\n                            for f in csv_files:\n                                lower_f = f.lower()\n                                # Avoid these unless they are the only option\n                                if any(x in lower_f for x in [\"atc\", \"inn\", \"text\", \"obal\", \"cest\", \"doping\", \"soli\", \"formy\", \"jednotky\", \"zavislost\", \"vydej\"]):\n                                    continue\n                                candidates.append(f)\n                            \n                            if candidates:\n                                # Pick one, ideally the one starting with dlp and looks like main file\n                                source_filename = candidates[0]\n                            else:\n                                # Fallback: largest CSV\n                                source_filename = max(csv_files, key=lambda x: z.getinfo(x).file_size)\n                        \n                    logger.info(f\"Selected {source_filename} for extraction as {target_filename}\")\n                    \n                    content = z.read(source_filename)\n                    target_path = self.download_dir / target_filename\n                    async with aiofiles.open(target_path, 'wb') as f:\n                        await f.write(content)\n            else:\n                # Direct file download (or save zip as is)\n                target_path = self.download_dir / target_filename\n                async with aiofiles.open(target_path, 'wb') as f:\n                    await f.write(resp.content)\n            \n            logger.info(f\"Downloaded {target_filename} to {self.download_dir}\")\n",
        "last_modified": "2025-12-25T21:22:14.879320"
      },
      "task_intent": {
        "title": "001-complete-guidelines-rag-pdf-import",
        "description": "",
        "from_plan": false
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2025-12-25T01:36:12.999874",
  "last_updated": "2025-12-25T01:36:13.013838"
}