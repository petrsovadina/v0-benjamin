{
  "file_path": ".auto-claude/specs/001-complete-guidelines-rag-pdf-import/spec.md",
  "main_branch_history": [],
  "task_views": {
    "001-complete-guidelines-rag-pdf-import": {
      "task_id": "001-complete-guidelines-rag-pdf-import",
      "branch_point": {
        "commit_hash": "d092d181e511c0ef7a36dcfd2dfe3f083cbae73e",
        "content": "",
        "timestamp": "2025-12-25T01:35:52.079046"
      },
      "worktree_state": {
        "content": "# Specification: Complete Guidelines RAG PDF Import\n\n## Overview\n\nImplement a robust PDF import pipeline for Czech medical guidelines that enables doctors to access evidence-based recommendations through an AI-powered system. This addresses a critical gap: Czech healthcare professionals lack access to localized clinical guidelines in general AI tools (ChatGPT, Claude), making it impossible to receive evidence-based recommendations with proper source citations. The system will allow admin users to upload guideline PDFs, automatically process them into searchable chunks with embeddings, and enable doctors to query this content with verifiable source attributions.\n\n## Workflow Type\n\n**Type**: feature\n\n**Rationale**: This is a new feature implementation that adds complete RAG (Retrieval-Augmented Generation) capabilities for medical guidelines. While a basic upload endpoint exists, this task requires building out the full pipeline, fixing schema mismatches, implementing retrieval logic, and ensuring production-grade reliability with multi-format support and comprehensive error handling.\n\n## Task Scope\n\n### Services Involved\n- **Backend (FastAPI)** (primary) - PDF upload endpoint, processing pipeline, vector storage, retrieval API\n- **Supabase** (integration) - Vector database storage with pgvector extension, metadata persistence\n\n### This Task Will:\n- [x] Verify and enhance the existing PDF upload endpoint at `/api/v1/admin/upload/guideline`\n- [x] Fix schema mismatch between chunk storage and guidelines table structure\n- [x] Implement proper metadata tracking (document name, page number) throughout the pipeline\n- [x] Test and validate support for at least 3 different Czech guideline PDF formats\n- [x] Add comprehensive logging for import progress and error handling\n- [x] Implement guideline retrieval in the RAG graph (currently TODO at line 74 of `backend/app/core/graph.py`)\n- [x] Add endpoint to query guidelines with proper source citations\n- [x] Create unit and integration tests for the pipeline\n\n### Out of Scope:\n- Frontend UI for admin PDF upload (will be implemented in future task)\n- User authentication and authorization (already exists)\n- Real-time processing status updates (background tasks are sufficient)\n- OCR for scanned PDFs (only text-based PDFs in scope)\n- Automatic guideline discovery/scraping from external sources\n\n## Service Context\n\n### Backend Service (FastAPI)\n\n**Tech Stack:**\n- Language: Python 3.13\n- Framework: FastAPI\n- Key libraries: langchain-community, langchain-text-splitters, langchain-openai, supabase, openai\n- Key directories:\n  - `backend/app/api/v1/endpoints/` - API endpoints\n  - `backend/data_processing/loaders/` - Document loaders\n  - `backend/services/` - Shared services (logger, search)\n  - `backend/data/guidelines_pdfs/` - PDF upload storage\n\n**Entry Point:** `backend/app/main.py`\n\n**How to Run:**\n```bash\n# From project root\ncd backend\nsource venv/bin/activate  # or venv\\Scripts\\activate on Windows\nuvicorn app.main:app --reload --port 8000\n```\n\n**Port:** 8000\n\n**Environment Variables Required:**\n```\nSUPABASE_URL=https://higziqzcjmtmkzxbbzik.supabase.co\nSUPABASE_KEY=<your-key>\nOPENAI_API_KEY=<your-key>\nANTHROPIC_API_KEY=<your-key>\nPUBMED_EMAIL=<your-email>\n```\n\n### Database Service (Supabase/PostgreSQL)\n\n**Tech Stack:**\n- Database: PostgreSQL with pgvector extension\n- Vector dimensions: 1536 (OpenAI text-embedding-3-small)\n- Index type: HNSW with cosine similarity\n\n**Key Tables:**\n- `guidelines` - Stores guideline chunks with embeddings (see migration 008_guidelines.sql)\n- `app_errors` - Error logging table\n\n**Migration File:** `supabase/migrations/008_guidelines.sql`\n\n## Files to Modify\n\n| File | Service | What to Change |\n|------|---------|---------------|\n| `backend/data_processing/loaders/guidelines_loader.py` | Backend | Fix schema mismatch - align chunk storage with guidelines table columns; improve metadata extraction |\n| `backend/app/api/v1/endpoints/admin.py` | Backend | Add validation, improve error handling, add status endpoint for tracking uploads |\n| `backend/app/core/graph.py` | Backend | Implement guideline retrieval node (replace TODO at line 74) |\n| `backend/app/services/search_service.py` | Backend | Add `search_guidelines()` method for vector similarity search |\n| `supabase/migrations/008_guidelines.sql` | Database | **CRITICAL CHANGES**: (1) Add `content TEXT NOT NULL` column for chunk text, (2) Add `metadata JSONB DEFAULT '{}'` column for page/source tracking, (3) Add `match_guidelines()` RPC function for vector search (current schema has only `full_content` - see migration 006 documents table for column patterns) |\n\n## Files to Reference\n\nThese files show patterns to follow:\n\n| File | Pattern to Copy |\n|------|----------------|\n| `backend/data_processing/loaders/guidelines_loader.py` | PDF loading with PyPDFLoader, chunking with RecursiveCharacterTextSplitter, embedding generation |\n| `backend/app/api/v1/endpoints/admin.py` | FastAPI file upload with BackgroundTasks, validation patterns |\n| `backend/services/logger.py` | StructuredLogger usage for JSON logging with metadata |\n| `backend/app/core/graph.py` | LangGraph state management, node implementation patterns |\n| `backend/app/api/v1/endpoints/query.py` | Citation formatting, source metadata handling |\n\n## Patterns to Follow\n\n### Pattern 1: Background Task Processing\n\nFrom `backend/app/api/v1/endpoints/admin.py`:\n\n```python\nasync def run_ingestion_task():\n    \"\"\"Background task to run the ingestion pipeline.\"\"\"\n    logger.info(\"Starting background ingestion task...\")\n    loader = GuidelinesLoader(pdf_dir=UPLOAD_DIR)\n    await loader.ingest_pdfs()\n    logger.info(\"Background ingestion task finished.\")\n\n@router.post(\"/upload/guideline\")\nasync def upload_guideline(\n    background_tasks: BackgroundTasks,\n    file: UploadFile = File(...)\n):\n    # Save file first\n    with open(file_path, \"wb\") as buffer:\n        shutil.copyfileobj(file.file, buffer)\n\n    # Then trigger background processing\n    background_tasks.add_task(run_ingestion_task)\n```\n\n**Key Points:**\n- Always save the uploaded file to disk BEFORE triggering background task (file stream closes)\n- Use FastAPI's BackgroundTasks for long-running operations\n- Return immediately with status \"uploaded\" and \"indexing started in background\"\n\n### Pattern 2: Chunking with Metadata Preservation\n\nFrom `backend/data_processing/loaders/guidelines_loader.py`:\n\n```python\nfrom langchain_community.document_loaders import PyPDFLoader\nfrom langchain_text_splitters import RecursiveCharacterTextSplitter\n\n# Load PDF (creates one Document per page)\nloader = PyPDFLoader(file_path)\ndocs = loader.load()\n\n# Split with context preservation\ntext_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000,\n    chunk_overlap=200,  # 20% overlap for context\n    separators=[\"\\n\\n\", \"\\n\", \" \", \"\"]\n)\nchunks = text_splitter.split_documents(docs)\n\n# Preserve metadata\nfor chunk in chunks:\n    metadata = {\n        \"source\": filename,\n        \"page\": chunk.metadata.get(\"page\", 0),  # 0-indexed\n        **chunk.metadata  # Preserve all original metadata\n    }\n```\n\n**Key Points:**\n- PyPDFLoader creates one Document per page with 0-indexed page numbers\n- RecursiveCharacterTextSplitter preserves Document metadata through splits\n- 20% chunk overlap (200 chars for 1000 char chunks) maintains context at boundaries\n- Always include source filename and page number in metadata\n\n### Pattern 3: Structured Logging\n\nFrom `backend/services/logger.py`:\n\n```python\nfrom backend.services.logger import get_logger\n\nlogger = get_logger(__name__)\n\n# Info logging with metadata\nlogger.info(f\"Processing {filename}...\",\n    file_size=os.path.getsize(file_path),\n    total_chunks=len(chunks)\n)\n\n# Error logging with exception\ntry:\n    # ... processing\nexcept Exception as e:\n    logger.error(f\"Failed to process {filename}\",\n        error=e,\n        filename=filename,\n        step=\"embedding_generation\"\n    )\n```\n\n**Key Points:**\n- Use `get_logger(__name__)` for module-scoped logging\n- Pass structured metadata as kwargs for JSON output\n- Error logs are automatically sent to Supabase `app_errors` table\n- Always include context (filename, step, etc.) in error logs\n\n### Pattern 4: Vector Similarity Search\n\nExpected pattern for `search_service.py`:\n\n```python\nfrom langchain_openai import OpenAIEmbeddings\nfrom backend.app.core.config import settings\n\nasync def search_guidelines(self, query: str, limit: int = 5) -> List[Dict[str, Any]]:\n    \"\"\"Search guidelines using vector similarity.\"\"\"\n    # 1. Generate query embedding\n    embeddings = OpenAIEmbeddings(\n        model=\"text-embedding-3-small\",\n        api_key=settings.OPENAI_API_KEY\n    )\n    embedding = embeddings.embed_query(query)\n\n    # 2. Search with RPC function (cosine similarity)\n    response = self.supabase.rpc('match_guidelines', {\n        'query_embedding': embedding,\n        'match_threshold': 0.7,\n        'match_count': limit\n    }).execute()\n\n    # 3. Format results with metadata\n    return [{\n        \"id\": item[\"id\"],\n        \"content\": item[\"content\"],\n        \"title\": item[\"title\"],\n        \"page\": item[\"metadata\"].get(\"page\"),\n        \"similarity\": item[\"similarity\"]\n    } for item in response.data]\n```\n\n**Key Points:**\n- Use RPC function for vector search (Supabase pattern)\n- Include similarity threshold (0.7-0.8 typical for medical content)\n- Return structured data with source metadata for citations\n\n**IMPORTANT - RPC Function Required:**\nThe `match_guidelines` RPC function must be created in the database migration. Add this SQL to `008_guidelines.sql`:\n\n```sql\n-- RPC function for vector similarity search on guidelines\nCREATE OR REPLACE FUNCTION match_guidelines(\n    query_embedding vector(1536),\n    match_threshold float,\n    match_count int\n)\nRETURNS TABLE (\n    id uuid,\n    title text,\n    content text,\n    metadata jsonb,\n    similarity float\n)\nLANGUAGE plpgsql\nAS $$\nBEGIN\n    RETURN QUERY\n    SELECT\n        guidelines.id,\n        guidelines.title,\n        guidelines.content,\n        guidelines.metadata,\n        1 - (guidelines.embedding <=> query_embedding) AS similarity\n    FROM guidelines\n    WHERE 1 - (guidelines.embedding <=> query_embedding) > match_threshold\n    ORDER BY guidelines.embedding <=> query_embedding\n    LIMIT match_count;\nEND;\n$$;\n```\n\n## Requirements\n\n### Functional Requirements\n\n1. **PDF Upload via Admin Endpoint**\n   - Description: Admin users can upload Czech medical guideline PDFs through POST `/api/v1/admin/upload/guideline`\n   - Acceptance:\n     - Endpoint accepts PDF files only (rejects other formats with 400 error)\n     - File is saved to `backend/data/guidelines_pdfs/` directory\n     - Returns immediately with status \"uploaded\" and background processing initiated\n     - Logs upload event with filename and file size\n\n2. **Document Parsing and Chunking**\n   - Description: PDFs are parsed into pages, then chunked into searchable segments\n   - Acceptance:\n     - Uses PyPDFLoader to extract text per page\n     - Chunks text with RecursiveCharacterTextSplitter (1000 chars, 200 overlap)\n     - Each chunk preserves source metadata (filename, page number)\n     - Logs chunk count per document\n\n3. **Embedding Generation**\n   - Description: Text chunks are converted to 1536-dimensional vectors using OpenAI's text-embedding-3-small model\n   - Acceptance:\n     - Embeddings generated in batches of 50 to avoid rate limits\n     - Each chunk has corresponding embedding vector\n     - Errors during embedding generation are logged and don't crash the pipeline\n     - Retry logic for transient API failures\n\n4. **Vector Database Storage**\n   - Description: Chunks with embeddings are stored in Supabase `guidelines` table\n   - Acceptance:\n     - Each chunk stored as separate row with vector embedding\n     - Database columns populated: `title` (filename), `organization`, `publication_year`, `content` (chunk text), `embedding` (1536-dim vector)\n     - Metadata JSONB column contains: `{\"source\": filename, \"page\": page_number, ...}` for citation tracking\n     - Duplicate prevention: delete existing chunks for filename before inserting new ones (idempotency via metadata->>source filter)\n     - HNSW index automatically used for similarity search\n\n5. **Guideline Retrieval in RAG Pipeline**\n   - Description: When query type is \"guidelines\", retrieve relevant chunks from vector database\n   - Acceptance:\n     - Graph classifier routes guideline queries to `retrieve_guidelines_node`\n     - Vector similarity search returns top 5 most relevant chunks\n     - Results include source metadata (title, page number) for citations\n     - Citations displayed to user in format: \"Source: [title], page [X]\"\n\n6. **Multi-Format PDF Support**\n   - Description: System handles at least 3 different Czech guideline PDF formats\n   - Acceptance:\n     - Test with standard text PDFs (e.g., MS Word \u2192 PDF exports)\n     - Test with PDFs containing tables (e.g., dosage tables in guidelines)\n     - Test with multi-column layouts (e.g., journal-style guidelines)\n     - Document any format-specific handling or limitations\n\n7. **Comprehensive Error Handling and Logging**\n   - Description: All pipeline stages log progress and handle errors gracefully\n   - Acceptance:\n     - Upload errors logged with file details\n     - Parsing errors logged with page number\n     - Embedding errors logged with retry count\n     - Database errors logged with SQL details\n     - All errors written to both stdout (JSON) and Supabase `app_errors` table\n     - Pipeline continues processing other files even if one fails\n\n### Edge Cases\n\n1. **Duplicate Uploads** - If same PDF uploaded twice, delete old chunks and replace with new ones (idempotency)\n2. **Empty PDFs** - If PDF has no text (scanned images), log warning and skip (return 0 chunks)\n3. **Very Large PDFs** - If PDF >500 pages, process in smaller batches to avoid memory issues\n4. **Malformed PDFs** - If PyPDFLoader fails, try fallback parser (e.g., pdfplumber) before failing\n5. **Non-Czech Content** - Mark `is_czech` flag based on language detection (currently defaults to True)\n6. **Missing Metadata** - If PDF lacks metadata (title, author), use filename as title and \"Unknown\" for organization\n\n## Implementation Notes\n\n### DO\n- Follow the existing GuidelinesLoader pattern in `backend/data_processing/loaders/guidelines_loader.py`\n- Use StructuredLogger from `backend/services/logger.py` for all logging\n- Reuse OpenAI embeddings configuration from `backend/app/core/config.py`\n- Process embeddings in batches of 50 to respect OpenAI rate limits\n- Include source filename and page number in every chunk's metadata JSONB field: `{\"source\": \"filename.pdf\", \"page\": 0}`\n- Delete existing chunks for a file before inserting new ones (idempotency via `metadata->>source` filter)\n- Use FastAPI's BackgroundTasks for long-running PDF processing\n- Add `content` and `metadata` columns to guidelines table (follow documents table pattern from migration 006)\n- Create `match_guidelines()` RPC function in migration for vector similarity search (SQL provided in Pattern 4)\n- Store chunks in the `guidelines` table with proper schema alignment\n- Implement retry logic for transient API failures (embeddings, database)\n\n### DON'T\n- Create new embedding models - use existing `OpenAIEmbeddings(model=\"text-embedding-3-small\")`\n- Block the upload endpoint waiting for processing to complete\n- Store the entire PDF in the database - only store chunks\n- Skip metadata tracking - citations depend on source attribution\n- Use generic error messages - include context (filename, step, error details)\n- Create a new logger implementation - use `get_logger(__name__)`\n- Modify the `guidelines` table schema without a migration file\n- Process all chunks in a single database transaction - use batches\n\n## Development Environment\n\n### Start Services\n\n```bash\n# Terminal 1: Start Frontend (Next.js)\nnpm run dev\n# Runs on http://localhost:3000\n\n# Terminal 2: Start Backend (FastAPI)\ncd backend\nsource venv/bin/activate  # Windows: venv\\Scripts\\activate\nuvicorn app.main:app --reload --port 8000\n# Runs on http://localhost:8000\n\n# Terminal 3: Supabase Local (optional for local testing)\nnpx supabase start\n# Vector DB available on local PostgreSQL\n```\n\n### Service URLs\n- Frontend: http://localhost:3000\n- Backend API: http://localhost:8000\n- API Docs: http://localhost:8000/docs (Swagger UI)\n- Supabase Studio: http://localhost:54323 (if running local)\n\n### Required Environment Variables\n\nCreate `backend/.env`:\n```bash\nSUPABASE_URL=https://higziqzcjmtmkzxbbzik.supabase.co\nSUPABASE_KEY=<your-service-role-key>\nOPENAI_API_KEY=<your-openai-key>\nANTHROPIC_API_KEY=<your-anthropic-key>\nPUBMED_EMAIL=<your-email>\n```\n\n### Test Data Preparation\n\nPlace test Czech guideline PDFs in:\n- `backend/data/guidelines_pdfs/` (upload destination)\n- `backend/tests/fixtures/` (test fixtures with at least 3 different formats)\n\n## Success Criteria\n\nThe task is complete when:\n\n1. [x] Admin can upload PDF via POST `/api/v1/admin/upload/guideline` with proper validation\n2. [x] PDF is parsed, chunked, embedded, and stored in `guidelines` table\n3. [x] Source metadata (filename, page number) preserved throughout pipeline\n4. [x] At least 3 different Czech guideline formats successfully processed\n5. [x] Guideline retrieval implemented in graph.py (TODO at line 74 resolved)\n6. [x] Queries classified as \"guidelines\" retrieve relevant chunks with citations\n7. [x] All processing stages logged with structured JSON output\n8. [x] Errors handled gracefully without crashing the pipeline\n9. [x] No console errors when uploading and querying guidelines\n10. [x] Existing tests still pass\n11. [x] New unit tests added for GuidelinesLoader and search_guidelines()\n12. [x] Integration test validates end-to-end flow: upload \u2192 process \u2192 retrieve\n\n## QA Acceptance Criteria\n\n**CRITICAL**: These criteria must be verified by the QA Agent before sign-off.\n\n### Unit Tests\n\n| Test | File | What to Verify |\n|------|------|----------------|\n| `test_guidelines_loader_chunk_creation` | `backend/tests/test_guidelines_loader.py` | Verify chunks created with correct metadata (source, page) |\n| `test_guidelines_loader_embedding_generation` | `backend/tests/test_guidelines_loader.py` | Verify embeddings are 1536-dimensional vectors |\n| `test_guidelines_loader_batch_processing` | `backend/tests/test_guidelines_loader.py` | Verify batch processing (50 chunks per batch) |\n| `test_search_guidelines_vector_similarity` | `backend/tests/test_search_service.py` | Verify search returns relevant results with similarity scores |\n| `test_upload_endpoint_pdf_validation` | `backend/tests/test_admin_endpoints.py` | Verify only PDFs accepted, other formats rejected with 400 |\n\n### Integration Tests\n\n| Test | Services | What to Verify |\n|------|----------|----------------|\n| `test_end_to_end_guideline_upload` | Backend \u2194 Supabase | Upload PDF \u2192 verify chunks in database with embeddings |\n| `test_guideline_retrieval_in_graph` | Backend \u2194 Supabase | Query with \"guidelines\" type \u2192 verify relevant chunks retrieved |\n| `test_citation_metadata_preservation` | Backend \u2194 Supabase | Upload \u2192 query \u2192 verify citations include source + page number |\n\n### End-to-End Tests\n\n| Flow | Steps | Expected Outcome |\n|------|-------|------------------|\n| Admin Upload & Doctor Query | 1. POST PDF to `/api/v1/admin/upload/guideline` 2. Wait for background processing (check logs) 3. POST query to `/api/v1/query/` with guideline question 4. Verify response includes citations | Citations format: \"Source: [filename], page [X]\"; Response includes relevant guideline content |\n| Multi-Format Support | 1. Upload 3 different PDF formats 2. Verify all processed successfully 3. Query content from each format | All 3 formats stored in database; Queries return content from all formats |\n| Error Recovery | 1. Upload invalid/corrupted PDF 2. Verify error logged 3. Verify pipeline continues 4. Upload valid PDF 5. Verify success | Error logged to `app_errors` table; Valid upload processes correctly |\n\n### Database Verification\n\n| Check | Query/Command | Expected |\n|-------|---------------|----------|\n| Guidelines table populated | `SELECT COUNT(*) FROM guidelines WHERE title = '[test-filename]'` | Count > 0 (chunks exist) |\n| Embeddings generated | `SELECT COUNT(*) FROM guidelines WHERE embedding IS NOT NULL` | Count matches total chunks |\n| HNSW index exists | `SELECT indexname FROM pg_indexes WHERE tablename = 'guidelines' AND indexname = 'idx_guidelines_embedding'` | Index exists |\n| Metadata preserved | `SELECT metadata FROM guidelines WHERE title = '[test-filename]' LIMIT 1` | metadata JSONB contains \"source\" and \"page\" keys |\n\n### API Verification\n\n| Endpoint | Method | Test Case | Expected Response |\n|----------|--------|-----------|-------------------|\n| `/api/v1/admin/upload/guideline` | POST | Upload valid PDF | 200, {\"status\": \"uploaded\", \"message\": \"...background...\"} |\n| `/api/v1/admin/upload/guideline` | POST | Upload .docx file | 400, {\"detail\": \"Only PDF files are supported.\"} |\n| `/api/v1/query/` | POST | Query: \"Jak\u00e9 jsou doporu\u010den\u00ed pro l\u00e9\u010dbu hypertenze?\" | 200, includes citations with \"guidelines\" source_type |\n\n### Logging Verification\n\n| Log Event | Expected Log Entry | Where to Check |\n|-----------|-------------------|----------------|\n| Upload started | `{\"level\": \"INFO\", \"message\": \"File saved to ...\", \"filename\": \"...\"}` | stdout (JSON) |\n| Processing started | `{\"level\": \"INFO\", \"message\": \"Processing ...\", \"total_chunks\": N}` | stdout (JSON) |\n| Error handling | `{\"level\": \"ERROR\", \"message\": \"Failed to process ...\", \"error_details\": \"...\"}` | stdout + `app_errors` table |\n| Processing complete | `{\"level\": \"INFO\", \"message\": \"Ingestion complete. Total chunks stored: N\"}` | stdout (JSON) |\n\n### QA Sign-off Requirements\n\n- [x] All unit tests pass (`pytest backend/tests/test_guidelines_loader.py -v`)\n- [x] All integration tests pass\n- [x] All E2E tests pass (upload \u2192 process \u2192 query \u2192 citations verified)\n- [x] Database verification complete (chunks stored, embeddings exist, metadata correct)\n- [x] API verification complete (upload endpoint validated, query endpoint returns citations)\n- [x] Logging verification complete (all events logged with proper structure)\n- [x] No regressions in existing functionality (all existing tests pass)\n- [x] Code follows established patterns (LangChain, StructuredLogger, FastAPI BackgroundTasks)\n- [x] No security vulnerabilities introduced (file validation, path traversal prevention)\n- [x] Multi-format support validated (3+ different PDF formats tested)\n- [x] Performance acceptable (embedding generation <5 min for 100-page PDF)\n- [x] Error handling tested (corrupt PDF, network failures, database errors)\n",
        "last_modified": "2025-12-25T21:22:14.786105"
      },
      "task_intent": {
        "title": "001-complete-guidelines-rag-pdf-import",
        "description": "",
        "from_plan": false
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2025-12-25T01:36:12.265577",
  "last_updated": "2025-12-25T01:36:12.279353"
}