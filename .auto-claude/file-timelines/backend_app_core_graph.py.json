{
  "file_path": "backend/app/core/graph.py",
  "main_branch_history": [],
  "task_views": {
    "001-complete-guidelines-rag-pdf-import": {
      "task_id": "001-complete-guidelines-rag-pdf-import",
      "branch_point": {
        "commit_hash": "d092d181e511c0ef7a36dcfd2dfe3f083cbae73e",
        "content": "",
        "timestamp": "2025-12-25T01:35:52.079046"
      },
      "worktree_state": {
        "content": "from typing import TypedDict, Annotated, Literal, List, Dict, Any\nfrom langgraph.graph import StateGraph, START, END\nfrom langgraph.graph.message import add_messages\nfrom langchain_core.messages import BaseMessage, HumanMessage, AIMessage, SystemMessage\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom backend.app.core.llm import get_llm\nfrom backend.app.services.search_service import search_service\nfrom pydantic import BaseModel, Field\n\n# --- STATE DEFINITION ---\nclass ClinicalState(TypedDict):\n    \"\"\"\n    Represents the state of a clinical query processing flow.\n    \"\"\"\n    messages: Annotated[list[BaseMessage], add_messages]\n    query_type: Literal[\"general\", \"drug_info\", \"guidelines\", \"clinical_trial\", \"reimbursement\", \"urgent\"] | None\n    retrieved_context: List[Dict[str, Any]]\n    final_answer: str | None\n    next_step: str | None\n\n# --- MODELS FOR CLASSIFICATION ---\nclass QueryClassification(BaseModel):\n    query_type: Literal[\"drug_info\", \"guidelines\", \"clinical\", \"urgent\", \"reimbursement\"] = Field(\n        ..., description=\"Type of clinical query based on content and intent.\"\n    )\n    reasoning: str = Field(..., description=\"Brief reasoning for the classification.\")\n\n# --- NODES ---\n\nasync def classifier_node(state: ClinicalState):\n    \"\"\"\n    Classifies the user query using LLM (or heuristic fallback).\n    \"\"\"\n    llm = get_llm()\n    last_msg = state[\"messages\"][-1].content\n    \n    if not llm:\n        # Fallback if no LLM configured/mock mode\n        lower_msg = last_msg.lower()\n        if \"l\u00e9k\" in lower_msg or \"sukl\" in lower_msg:\n            return {\"query_type\": \"drug_info\", \"next_step\": \"retrieve_drugs\"}\n        if any(kw in lower_msg for kw in [\"guideline\", \"doporu\u010den\u00ed\", \"protokol\", \"standard\", \"postup\"]):\n            return {\"query_type\": \"guidelines\", \"next_step\": \"retrieve_guidelines\"}\n        return {\"query_type\": \"clinical\", \"next_step\": \"retrieve_general\"}\n\n    # Structured output classification\n    structured_llm = llm.with_structured_output(QueryClassification)\n    \n    classification_prompt = ChatPromptTemplate.from_messages([\n        (\"system\", \"\"\"You are a classification system for a medical assistant. \n        Analyze the query and determine the best category:\n        1. drug_info: Specific drug questions, dosage, interactions, S\u00daKL, prices.\n        2. guidelines: Requests for clinical guidelines, protocols, standards.\n        3. clinical: General clinical questions, diagnosis, symptoms, treatment options.\n        4. urgent: Emergency situations, life-threatening conditions (AIM, CPR).\n        5. reimbursement: Insurance coverage, VZP conditions.\n        \"\"\"),\n        (\"user\", \"{query}\")\n    ])\n    \n    try:\n        result = await structured_llm.ainvoke(classification_prompt.format(query=last_msg))\n        q_type = result.query_type\n    except Exception as e:\n        # Fallback on error\n        print(f\"Classification error: {e}\")\n        q_type = \"clinical\"\n\n    # Map to next step\n    if q_type == \"drug_info\":\n        next_step = \"retrieve_drugs\"\n    elif q_type == \"reimbursement\":\n        # S\u00daKL data often contains reimbursement info\n        next_step = \"retrieve_drugs\" \n    elif q_type == \"guidelines\":\n        # Route to dedicated guidelines retrieval (vector similarity search)\n        next_step = \"retrieve_guidelines\"\n    elif q_type == \"urgent\":\n        # Urgent queries might skip complex RAG or use specific \"emergency\" RAG\n        next_step = \"retrieve_general\"\n    else:\n        next_step = \"retrieve_general\"\n\n    return {\"query_type\": q_type, \"next_step\": next_step}\n\nasync def retrieve_drugs_node(state: ClinicalState):\n    \"\"\"\n    Retrieves drug information using SearchService (S\u00daKL).\n    \"\"\"\n    query = state[\"messages\"][-1].content\n    drugs = await search_service.search_drugs(query)\n    \n    # Format context\n    context_str = \"\"\n    raw_data = []\n    for d in drugs:\n        context_str += f\"L\u00e9k: {d.get('name')} (S\u00daKL: {d.get('sukl_code')})\\n\"\n        context_str += f\"\u00da\u010dinn\u00e1 l\u00e1tka: {d.get('active_substance')}\\n\"\n        context_str += f\"Dostupnost: {'Dostupn\u00fd' if d.get('is_available') else 'Nedostupn\u00fd'}\\n\\n\"\n        raw_data.append({\"source\": \"sukl\", \"data\": d})\n    \n    return {\"retrieved_context\": raw_data}\n\nasync def retrieve_general_node(state: ClinicalState):\n    \"\"\"\n    Retrieves literature using SearchService (PubMed).\n    \"\"\"\n    query = state[\"messages\"][-1].content\n    # Depending on query type, we might adjust queries (e.g. add \"guidelines\" content)\n    papers = await search_service.search_pubmed(query, max_results=3)\n\n    raw_data = []\n    for p in papers:\n        raw_data.append({\"source\": \"pubmed\", \"data\": p})\n\n    return {\"retrieved_context\": raw_data}\n\nasync def retrieve_guidelines_node(state: ClinicalState):\n    \"\"\"\n    Retrieves clinical guidelines using SearchService (vector similarity search).\n    Returns guideline chunks with source metadata for citations.\n    \"\"\"\n    query = state[\"messages\"][-1].content\n    guidelines = await search_service.search_guidelines(query, limit=5)\n\n    raw_data = []\n    for g in guidelines:\n        raw_data.append({\"source\": \"guidelines\", \"data\": g})\n\n    return {\"retrieved_context\": raw_data}\n\nasync def synthesizer_node(state: ClinicalState):\n    \"\"\"\n    Synthesizes the final answer using the Retrieved Context and System Prompt.\n    \"\"\"\n    llm = get_llm()\n    if not llm:\n        return {\"final_answer\": \"LLM not configured.\"}\n        \n    context = state.get(\"retrieved_context\", [])\n    query_type = state.get(\"query_type\", \"clinical\")\n    \n    # Construct Context String for LLM\n    context_text = \"NALEZEN\u00c9 ZDROJE:\\n\\n\"\n    citations_data = []\n    \n    for idx, item in enumerate(context, 1):\n        data = item[\"data\"]\n        source = item[\"source\"]\n        \n        if source == \"sukl\":\n            context_text += f\"[{idx}] S\u00daKL: {data.get('name')}\\n{data}\\n\\n\"\n            citations_data.append(f\"[{idx}] S\u00daKL - {data.get('name')} (K\u00f3d: {data.get('sukl_code')})\")\n        elif source == \"pubmed\":\n            context_text += f\"[{idx}] PubMed: {data.get('title')}\\nAbstract: {data.get('abstract')}\\nUrl: {data.get('url')}\\n\\n\"\n            citations_data.append(f\"[{idx}] {data.get('authors')[0] if data.get('authors') else 'Unknown'} et al. {data.get('title')}. {data.get('url')}\")\n        elif source == \"guidelines\":\n            # Format guideline chunks with source and page info for citations\n            # Format: 'Source: [filename], page [X]' as per spec\n            guideline_source = data.get('source', 'Klinick\u00e1 doporu\u010den\u00ed')\n            page_num = data.get('page', '')\n            content = data.get('content', data.get('text', ''))\n            page_info = f\", page {page_num}\" if page_num else \"\"\n            context_text += f\"[{idx}] Source: {guideline_source}{page_info}\\n{content}\\n\\n\"\n            citations_data.append(f\"[{idx}] Source: {guideline_source}{page_info}\")\n    \n    # System Prompt (simplified version of the full spec for code brevity, \n    # but capturing the key 'Identity' and 'Principles')\n    system_prompt_text = \"\"\"Jsi Czech MedAI \u2014 d\u016fv\u011bryhodn\u00fd AI asistent pro \u010desk\u00e9 zdravotnick\u00e9 profesion\u00e1ly.\n    \n    Z\u00c1KLADN\u00cd PRINCIPY:\n    1. Evidence-based: Odpov\u011bdi podlo\u017een\u00e9 citacemi [1][2].\n    2. Transparentnost: Uv\u00e1d\u011bj zdroje.\n    3. \u010cesk\u00e1 lokalizace: Pou\u017e\u00edvej \u010desk\u00e9 guidelines a terminologii (TK, DM).\n    4. Bezpe\u010dnost: Neposkytuj diagn\u00f3zy, jen informace. P\u0159i akutn\u00edch stavech (AIM, CMP) varuj.\n    \n    FORM\u00c1T ODPOV\u011aDI:\n    1. P\u0159\u00edm\u00e1 odpov\u011b\u010f s inline citacemi.\n    2. Seznam citac\u00ed na konci.\n    \n    Pou\u017eij poskytnut\u00fd kontext k zodpov\u011bzen\u00ed dotazu. Pokud kontext nesta\u010d\u00ed, p\u0159iznej to.\n    \"\"\"\n    \n    messages = [\n        SystemMessage(content=system_prompt_text),\n        HumanMessage(content=f\"DOTAZ: {state['messages'][-1].content}\\n\\n{context_text}\")\n    ]\n    \n    response = await llm.ainvoke(messages)\n    \n    return {\"final_answer\": response.content}\n\n# --- GRAPH CONSTRUCTION ---\nworkflow = StateGraph(ClinicalState)\n\nworkflow.add_node(\"classifier\", classifier_node)\nworkflow.add_node(\"retrieve_drugs\", retrieve_drugs_node)\nworkflow.add_node(\"retrieve_general\", retrieve_general_node)\nworkflow.add_node(\"retrieve_guidelines\", retrieve_guidelines_node)\nworkflow.add_node(\"synthesizer\", synthesizer_node)\n\nworkflow.add_edge(START, \"classifier\")\n\ndef route_query(state: ClinicalState):\n    return state[\"next_step\"]\n\nworkflow.add_conditional_edges(\n    \"classifier\",\n    route_query,\n    {\n        \"retrieve_drugs\": \"retrieve_drugs\",\n        \"retrieve_general\": \"retrieve_general\",\n        \"retrieve_guidelines\": \"retrieve_guidelines\"\n    }\n)\n\nworkflow.add_edge(\"retrieve_drugs\", \"synthesizer\")\nworkflow.add_edge(\"retrieve_general\", \"synthesizer\")\nworkflow.add_edge(\"retrieve_guidelines\", \"synthesizer\")\nworkflow.add_edge(\"synthesizer\", END)\n\napp = workflow.compile()\n",
        "last_modified": "2025-12-25T21:22:14.850691"
      },
      "task_intent": {
        "title": "001-complete-guidelines-rag-pdf-import",
        "description": "",
        "from_plan": false
      },
      "commits_behind_main": 0,
      "status": "active",
      "merged_at": null
    }
  },
  "created_at": "2025-12-25T01:36:12.747957",
  "last_updated": "2025-12-25T01:36:12.760117"
}